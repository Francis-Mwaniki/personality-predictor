{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9kgP+R6kuAwGsfEctVNxj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Francis-Mwaniki/personality-predictor/blob/main/product_recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOcVMPJCZOeo",
        "outputId": "38e85bbb-0638-4d68-f5d8-524ac3234a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro-averaged F1 Score: 1.0000\n",
            "Macro-averaged F1 Score: 0.0417\n",
            "Hamming Loss: 0.0000\n",
            "Recommended products: ['Current Accounts']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import f1_score, hamming_loss\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('test.csv')\n",
        "\n",
        "# Select features and target variables\n",
        "features = ['age', 'seniority', 'income', 'sex', 'segment', 'cust_type', 'residence_index', 'foreigner_index']\n",
        "products = ['Saving Account', 'Guarantees', 'Current Accounts', 'Derivada Account', 'Payroll Account',\n",
        "            'Junior Account', 'Más particular Account', 'particular Account', 'particular Plus Account',\n",
        "            'Short-term deposits', 'Medium-term deposits', 'Long-term deposits', 'e-account', 'Funds',\n",
        "            'Mortgage', 'Pensions', 'Loans', 'Taxes', 'Credit Card', 'Securities', 'Home Account', 'Payroll',\n",
        "            'Pensions', 'Direct Debit']\n",
        "\n",
        "# Preprocess the data\n",
        "le = LabelEncoder()\n",
        "for col in features:\n",
        "    if df[col].dtype == 'object':\n",
        "        df[col] = le.fit_transform(df[col].astype(str))\n",
        "\n",
        "# Split the data\n",
        "X = df[features]\n",
        "y = df[products]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train the model\n",
        "model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "h_loss = hamming_loss(y_test, y_pred)\n",
        "\n",
        "print(f\"Micro-averaged F1 Score: {f1_micro:.4f}\")\n",
        "print(f\"Macro-averaged F1 Score: {f1_macro:.4f}\")\n",
        "print(f\"Hamming Loss: {h_loss:.4f}\")\n",
        "\n",
        "# Function to predict products for a new customer\n",
        "def predict_products(customer_data):\n",
        "    # Ensure customer_data has the same features in the same order\n",
        "    customer_array = np.array(customer_data).reshape(1, -1)\n",
        "    customer_scaled = scaler.transform(customer_array)\n",
        "    predictions = model.predict(customer_scaled)\n",
        "    recommended_products = [products[i] for i, pred in enumerate(predictions[0]) if pred == 1]\n",
        "    return recommended_products\n",
        "\n",
        "# Example usage\n",
        "new_customer = [35, 6, 87218.1, 0, 2, 1, 1, 0]  # Example values for features\n",
        "recommended = predict_products(new_customer)\n",
        "print(\"Recommended products:\", recommended)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Improved**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-b8dcCbDZoVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import f1_score, hamming_loss, make_scorer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('test.csv')\n",
        "\n",
        "# Select features and target variables\n",
        "features = ['age', 'seniority', 'income', 'sex', 'segment', 'cust_type', 'residence_index', 'foreigner_index']\n",
        "products = ['Saving Account', 'Guarantees', 'Current Accounts', 'Derivada Account', 'Payroll Account',\n",
        "            'Junior Account', 'Más particular Account', 'particular Account', 'particular Plus Account',\n",
        "            'Short-term deposits', 'Medium-term deposits', 'Long-term deposits', 'e-account', 'Funds',\n",
        "            'Mortgage', 'Pensions', 'Loans', 'Taxes', 'Credit Card', 'Securities', 'Home Account', 'Payroll',\n",
        "            'Pensions', 'Direct Debit']\n",
        "\n",
        "# Preprocess the data\n",
        "le = LabelEncoder()\n",
        "for col in features:\n",
        "    if df[col].dtype == 'object':\n",
        "        df[col] = le.fit_transform(df[col].astype(str))\n",
        "\n",
        "# Split the data\n",
        "X = df[features]\n",
        "y = df[products]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Feature selection\n",
        "selector = SelectKBest(score_func=f_classif, k=5)\n",
        "X_train_selected = selector.fit_transform(X_train_scaled, y_train.sum(axis=1))\n",
        "X_test_selected = selector.transform(X_test_scaled)\n",
        "\n",
        "# Train the model\n",
        "base_model = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "model = MultiOutputClassifier(base_model)\n",
        "model.fit(X_train_selected, y_train)\n",
        "\n",
        "# Cross-validation\n",
        "cv_score = cross_val_score(model, X_train_selected, y_train, cv=5, scoring=make_scorer(f1_score, average='micro'))\n",
        "print(f\"Cross-validation F1 score: {cv_score.mean():.4f} (+/- {cv_score.std() * 2:.4f})\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_selected)\n",
        "\n",
        "# Evaluate the model\n",
        "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
        "h_loss = hamming_loss(y_test, y_pred)\n",
        "\n",
        "print(f\"Micro-averaged F1 Score: {f1_micro:.4f}\")\n",
        "print(f\"Macro-averaged F1 Score: {f1_macro:.4f}\")\n",
        "print(f\"Hamming Loss: {h_loss:.4f}\")\n",
        "\n",
        "# Function to predict products for a new customer\n",
        "def predict_products(customer_data):\n",
        "    customer_array = np.array(customer_data).reshape(1, -1)\n",
        "    customer_scaled = scaler.transform(customer_array)\n",
        "    customer_selected = selector.transform(customer_scaled)\n",
        "    predictions = model.predict(customer_selected)\n",
        "    recommended_products = [products[i] for i, pred in enumerate(predictions[0]) if pred == 1]\n",
        "    return recommended_products\n",
        "\n",
        "# Example usage\n",
        "new_customer = [35, 6, 87218.1, 0, 2, 1, 1, 0]  # Example values for features\n",
        "recommended = predict_products(new_customer)\n",
        "print(\"Recommended products:\", recommended)\n",
        "\n",
        "# Print feature importances\n",
        "selected_features = selector.get_support(indices=True)\n",
        "for i, feature in enumerate(np.array(features)[selected_features]):\n",
        "    print(f\"Selected feature: {feature}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tb1YY5mxcWyz",
        "outputId": "e426b46f-fa6a-413e-8653-75600577af45"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_univariate_selection.py:108: RuntimeWarning: invalid value encountered in divide\n",
            "  msb = ssbn / float(dfbn)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "1 fits failed out of a total of 5.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "1 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/multioutput.py\", line 450, in fit\n",
            "    super().fit(X, Y, sample_weight, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/multioutput.py\", line 216, in fit\n",
            "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\", line 63, in __call__\n",
            "    return super().__call__(iterable_with_config)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\", line 1918, in __call__\n",
            "    return output if self.return_generator else list(output)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
            "    res = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\", line 123, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/multioutput.py\", line 49, in _fit_estimator\n",
            "    estimator.fit(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 730, in inner_f\n",
            "    return func(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\", line 1471, in fit\n",
            "    raise ValueError(\n",
            "ValueError: Invalid classes inferred from unique values of `y`.  Expected: [0], got [1]\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation F1 score: nan (+/- nan)\n",
            "Micro-averaged F1 Score: 1.0000\n",
            "Macro-averaged F1 Score: 0.0417\n",
            "Hamming Loss: 0.0000\n",
            "Recommended products: ['Current Accounts']\n",
            "Selected feature: sex\n",
            "Selected feature: segment\n",
            "Selected feature: cust_type\n",
            "Selected feature: residence_index\n",
            "Selected feature: foreigner_index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import f1_score, hamming_loss, make_scorer\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('test.csv')\n",
        "\n",
        "# Select features and target variables\n",
        "features = ['age', 'seniority', 'income', 'sex', 'segment', 'cust_type', 'residence_index', 'foreigner_index']\n",
        "products = ['Saving Account', 'Guarantees', 'Current Accounts', 'Derivada Account', 'Payroll Account',\n",
        "            'Junior Account', 'Más particular Account', 'particular Account', 'particular Plus Account',\n",
        "            'Short-term deposits', 'Medium-term deposits', 'Long-term deposits', 'e-account', 'Funds',\n",
        "            'Mortgage', 'Pensions', 'Loans', 'Taxes', 'Credit Card', 'Securities', 'Home Account', 'Payroll',\n",
        "            'Pensions', 'Direct Debit']\n",
        "\n",
        "# Preprocess the data\n",
        "le = LabelEncoder()\n",
        "for col in features:\n",
        "    if df[col].dtype == 'object':\n",
        "        df[col] = le.fit_transform(df[col].astype(str))\n",
        "\n",
        "# Split the data\n",
        "X = df[features]\n",
        "y = df[products]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "X_test_scaled = scaler.transform(X_test_imputed)\n",
        "\n",
        "# Feature selection (remove low variance features)\n",
        "selector = VarianceThreshold(threshold=0.01)\n",
        "X_train_selected = selector.fit_transform(X_train_scaled)\n",
        "X_test_selected = selector.transform(X_test_scaled)\n",
        "\n",
        "# Train the model\n",
        "base_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model = MultiOutputClassifier(base_model)\n",
        "model.fit(X_train_selected, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_selected)\n",
        "\n",
        "# Evaluate the model\n",
        "f1_micro = f1_score(y_test, y_pred, average='micro', zero_division=1)\n",
        "f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=1)\n",
        "h_loss = hamming_loss(y_test, y_pred)\n",
        "\n",
        "print(f\"Micro-averaged F1 Score: {f1_micro:.4f}\")\n",
        "print(f\"Macro-averaged F1 Score: {f1_macro:.4f}\")\n",
        "print(f\"Hamming Loss: {h_loss:.4f}\")\n",
        "\n",
        "# Function to predict products for a new customer\n",
        "def predict_products(customer_data):\n",
        "    customer_array = np.array(customer_data).reshape(1, -1)\n",
        "    customer_imputed = imputer.transform(customer_array)\n",
        "    customer_scaled = scaler.transform(customer_imputed)\n",
        "    customer_selected = selector.transform(customer_scaled)\n",
        "    predictions = model.predict(customer_selected)\n",
        "    recommended_products = [products[i] for i, pred in enumerate(predictions[0]) if pred == 1]\n",
        "    return recommended_products\n",
        "\n",
        "# Example usage\n",
        "new_customer = [35, 6, 87218.1, 0, 2, 1, 1, 0]  # Example values for features\n",
        "recommended = predict_products(new_customer)\n",
        "print(\"Recommended products:\", recommended)\n",
        "\n",
        "# Print selected features\n",
        "selected_features = np.array(features)[selector.get_support()]\n",
        "print(\"Selected features:\", selected_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKNwrfEkdKjA",
        "outputId": "0540b35f-56ca-4b1a-f7b6-060561120843"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Micro-averaged F1 Score: 1.0000\n",
            "Macro-averaged F1 Score: 1.0000\n",
            "Hamming Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but SimpleImputer was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommended products: ['Current Accounts']\n",
            "Selected features: ['age' 'income' 'sex' 'segment']\n"
          ]
        }
      ]
    }
  ]
}